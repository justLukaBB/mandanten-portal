---
phase: 09-multi-page-extraction
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - /tmp/creditor-fastapi/app/services/document_processor.py
  - /tmp/creditor-fastapi/app/routers/processing.py
autonomous: true

must_haves:
  truths:
    - "page_count flows from _validate_pdf() through process_document() to extract_data()"
    - "PDF with 0 creditors returns error status instead of empty success"
    - "Multi-creditor split preserves page data in each creditor's extracted_data"
    - "Webhook results for PDF creditors use identical structure (source_document_id, creditor_index, creditor_count) as image creditors"
    - "Single image processing works identically to before (no regressions)"
  artifacts:
    - path: "/tmp/creditor-fastapi/app/services/document_processor.py"
      provides: "page_count threading and zero-creditor error handling"
      contains: "page_count"
    - path: "/tmp/creditor-fastapi/app/routers/processing.py"
      provides: "Page data flowing through multi-creditor split"
      contains: "pages"
  key_links:
    - from: "/tmp/creditor-fastapi/app/services/document_processor.py"
      to: "extract_data()"
      via: "page_count parameter passed from process_document"
      pattern: "extract_data.*page_count"
    - from: "/tmp/creditor-fastapi/app/routers/processing.py"
      to: "/tmp/creditor-fastapi/app/services/document_processor.py"
      via: "process_document call unchanged, page_count handled internally"
      pattern: "process_document"
    - from: "/tmp/creditor-fastapi/app/routers/processing.py"
      to: "webhook payload"
      via: "creditor_result.extracted_data contains page data from CreditorData"
      pattern: "creditor_extracted_data"
---

<objective>
Wire page_count through the processing pipeline and handle edge cases. Thread page_count from _validate_pdf() through process_document() to extract_data(). Add zero-creditor error handling for PDFs. Ensure page data flows through the multi-creditor split in processing.py so webhook results include page assignments. Verify backward compatibility with single image processing.

Purpose: Connects Plan 01's extraction capability to the full pipeline. Without this wiring, page_count never reaches extract_data() and page data never reaches the webhook.
Output: Complete end-to-end page assignment pipeline from PDF upload to webhook result.
</objective>

<execution_context>
@/Users/luka.s/.claude/get-shit-done/workflows/execute-plan.md
@/Users/luka.s/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-multi-page-extraction/09-RESEARCH.md
@.planning/phases/09-multi-page-extraction/09-CONTEXT.md
@.planning/phases/09-multi-page-extraction/09-01-SUMMARY.md

Key source files (in separate FastAPI repository at /tmp/creditor-fastapi):
@/tmp/creditor-fastapi/app/services/document_processor.py
@/tmp/creditor-fastapi/app/routers/processing.py
@/tmp/creditor-fastapi/app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Thread page_count through process_document to extract_data and add zero-creditor error</name>
  <files>
    /tmp/creditor-fastapi/app/services/document_processor.py
  </files>
  <action>
**Step A: Capture page_count in process_document()**

In the `process_document()` method, the `is_pdf` variable is already computed at Step 3 (around line 808). After the `is_pdf` check, capture the page_count for PDFs:

```python
# Step 3: Rotation analysis and correction (images only - PDFs don't need rotation)
is_pdf = os.path.splitext(image_path)[1].lower() == '.pdf' or mime_type == 'application/pdf'
page_count = None  # Add this line
if is_pdf:
    logger.info(f"Skipping rotation analysis for PDF: {filename}")
    result.rotation = RotationResult(
        should_rotate=False,
        rotation_angle="0",
        reason="Rotation analysis skipped for PDF documents"
    )
    # Get page count for extraction prompt (re-read from file since _validate_pdf was called in _load_image_as_part)
    try:
        with open(image_path, 'rb') as f:
            pdf_reader = PdfReader(BytesIO(f.read()))
            page_count = len(pdf_reader.pages)
        logger.info(f"PDF page count for extraction: {page_count}")
    except Exception as e:
        logger.warning(f"Could not get PDF page count: {e}")
else:
    # ... existing rotation code ...
```

Note: `PdfReader` and `BytesIO` are already imported at the top of the file (added in Phase 8).

**Step B: Pass page_count to extract_data()**

In Step 4 of `process_document()` (around line 822-824), update the `extract_data()` call to pass `page_count`:

Change:
```python
extracted_data = self.extract_data(image_path)
```
To:
```python
extracted_data = self.extract_data(image_path, page_count=page_count)
```

This is safe because `page_count` defaults to `None` for images, and `extract_data()` (modified in Plan 01) only appends PDF instructions when `is_pdf and page_count` is truthy.

**Step C: Add zero-creditor error handling for PDFs**

After the extraction and non-creditor checks (after the `if ai_says_not_creditor or no_data_found:` block, around line 857), add a new check for PDFs with zero creditors:

```python
# Check for zero creditors in PDF (per CONTEXT.md decision: return error)
if is_pdf and extracted_data.is_creditor_document and len(extracted_data.creditors) == 0:
    logger.warning(f"PDF document classified as creditor but 0 creditors extracted: {filename}")
    result.status = DocumentStatus.EXTRACTION_ERROR
    result.processing_status = "error"
    result.document_status = "needs_review"
    result.manual_review_required = True
    result.processing_error = "No creditors found in PDF document"
    result.extracted_data = ExtractedData(
        error="No creditors found in PDF document",
        is_creditor_document=True,
        manual_review_required=True,
        timestamp=datetime.utcnow().isoformat()
    )
    result.processing_time_ms = int((time.time() - start_time) * 1000)
    return result
```

Place this AFTER the `ai_says_not_creditor or no_data_found` block but BEFORE Step 5 (verification). This way:
- If Gemini says NOT a creditor doc -> handled by existing logic (non_creditor status)
- If Gemini says IS a creditor doc but returns 0 creditors -> new error (zero creditors in PDF)
- If Gemini returns creditors normally -> continues to verification

**IMPORTANT:** This zero-creditor check is ONLY for PDFs (`if is_pdf and ...`). Single images that return a creditor array already have at least one creditor or are caught by the `no_data_found` check.
  </action>
  <verify>
1. Run `cd /tmp/creditor-fastapi && python -c "from app.services.document_processor import DocumentProcessor; print('import OK')"` -- should succeed
2. Grep for `page_count` in process_document to confirm it's threaded through: `grep -n 'page_count' /tmp/creditor-fastapi/app/services/document_processor.py`
3. Grep for zero-creditor error: `grep -n 'No creditors found' /tmp/creditor-fastapi/app/services/document_processor.py`
4. Grep for extract_data call with page_count: `grep -n 'extract_data.*page_count' /tmp/creditor-fastapi/app/services/document_processor.py`
  </verify>
  <done>
- page_count is captured from PDF in process_document() and passed to extract_data()
- Zero-creditor error returns EXTRACTION_ERROR status with clear error message for PDFs
- Image processing is unchanged (page_count=None, no PDF checks triggered)
  </done>
</task>

<task type="auto">
  <name>Task 2: Ensure page data flows through multi-creditor split and verify COMPAT-02</name>
  <files>
    /tmp/creditor-fastapi/app/routers/processing.py
  </files>
  <action>
**Verify page data flows through existing split logic (processing.py)**

The multi-creditor split logic in `processing.py` (lines 226-348) creates separate `DocumentResult` entries for each creditor. It already copies the `CreditorData` object directly:

```python
creditor_extracted_data = ExtractedData(
    creditor_data=creditor,  # This IS the CreditorData object with .pages field
    ...
)
```

Since `creditor` is the `CreditorData` object (which now has the `pages` field from Plan 01), the page data automatically flows through to `creditor_extracted_data.creditor_data.pages`. **No changes to the split logic are needed** -- the page data is preserved because `CreditorData` is passed by reference.

However, for explicit observability, add a log line in the split loop (around line 332, after the existing `logger.info` for each creditor):

After the existing line:
```python
logger.info(f"  [{idx}/{creditor_count}] {creditor_name} - Review: {creditor_needs_review}")
```

Add:
```python
if creditor.pages:
    logger.info(f"    Pages: {creditor.pages}")
```

**Verify COMPAT-02 compliance:**

The webhook payload structure is determined by `DocumentResult.model_dump()` which serializes to JSON. The existing fields `source_document_id`, `creditor_index`, `creditor_count` are already on `DocumentResult` and set correctly in the split logic. The page data is nested inside `extracted_data.creditor_data.pages`, which serializes automatically via Pydantic.

Verify this is correct by checking that:
1. `creditor_result.source_document_id` is set (already done at line 264)
2. `creditor_result.creditor_index` is set (already done at line 265)
3. `creditor_result.creditor_count` is set (already done at line 266)
4. `creditor_result.extracted_data.creditor_data` contains the CreditorData with pages (already done via `creditor_extracted_data`)

**No structural changes needed to processing.py beyond the log line.** The COMPAT-02 requirement is satisfied by the existing split logic which already creates identical structures for multi-creditor results regardless of source (image or PDF).

If you find the `creditor.pages` attribute is not accessible (because the CreditorData objects are created in extract_data before Plan 01's changes), verify that the CreditorData import in processing.py pulls the updated model. This should work because Python imports reference the module, not a snapshot.
  </action>
  <verify>
1. Run `cd /tmp/creditor-fastapi && python -c "
from app.models import CreditorData, ExtractedData, DocumentResult
# Simulate what happens in multi-creditor split
creditor = CreditorData(sender_name='Test Bank', pages=[1, 2, 3])
extracted = ExtractedData(creditor_data=creditor, is_creditor_document=True)
result = DocumentResult(
    filename='test.pdf',
    source_document_id='doc_abc123',
    creditor_index=1,
    creditor_count=3,
    extracted_data=extracted
)
# Verify page data survives serialization
data = result.model_dump()
pages = data['extracted_data']['creditor_data']['pages']
assert pages == [1, 2, 3], f'Expected [1,2,3], got {pages}'
assert data['source_document_id'] == 'doc_abc123'
assert data['creditor_index'] == 1
assert data['creditor_count'] == 3
print('COMPAT-02 verified: page data + multi-creditor fields serialize correctly')
"` -- should print verification message

2. Run `cd /tmp/creditor-fastapi && python -c "
from app.models import CreditorData, ExtractedData, DocumentResult
# Verify backward compat: image creditor with no pages
creditor = CreditorData(sender_name='Image Creditor')
assert creditor.pages == [], f'Expected empty, got {creditor.pages}'
extracted = ExtractedData(creditor_data=creditor, is_creditor_document=True)
result = DocumentResult(filename='test.jpg', extracted_data=extracted)
data = result.model_dump()
assert data['extracted_data']['creditor_data']['pages'] == []
assert data['source_document_id'] is None
print('Backward compat verified: image creditors have empty pages, no source_document_id')
"` -- should print verification message

3. Grep for the new log line: `grep -n 'Pages:' /tmp/creditor-fastapi/app/routers/processing.py`
  </verify>
  <done>
- Page data flows through multi-creditor split via CreditorData.pages field (no structural changes needed)
- Observability log added showing page assignments during split
- COMPAT-02 verified: webhook payload includes source_document_id, creditor_index, creditor_count, and pages data serializes correctly
- Backward compatibility verified: image creditors have empty pages list, no source_document_id
  </done>
</task>

</tasks>

<verification>
1. Full import chain works: `python -c "from app.routers.processing import router; print('OK')"`
2. CreditorData.pages serializes in webhook payload
3. page_count threads from _validate_pdf -> process_document -> extract_data
4. Zero-creditor PDF returns error status
5. Image processing is completely unchanged (page_count=None, pages=[])
6. Multi-creditor split preserves page assignments
7. COMPAT-02: source_document_id + creditor_index + creditor_count + pages all present in serialized output
</verification>

<success_criteria>
- PDF processing: page_count reaches extract_data, page instructions appear in prompt, page data appears in CreditorData
- Zero creditors from PDF: returns EXTRACTION_ERROR with "No creditors found in PDF document"
- Multi-creditor split: page data preserved in each split creditor's extracted_data
- COMPAT-02: webhook results use identical structure (source_document_id, creditor_index, creditor_count) for both images and PDFs
- Backward compat: single image processing works identically (page_count=None, pages=[], no PDF checks)
</success_criteria>

<output>
After completion, create `.planning/phases/09-multi-page-extraction/09-02-SUMMARY.md`
</output>
